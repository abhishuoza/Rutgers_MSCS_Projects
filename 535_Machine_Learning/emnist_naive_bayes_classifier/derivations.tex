\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Derivations for Categorical Naive Bayes: MLE and MAP Estimates}
\author{}
\date{}

\begin{document}

\maketitle

\section{MLE for Class Prior $\theta_k$}

\textbf{Goal:} Find $\theta_k = P(y = k)$ that maximizes the likelihood of observed class labels.

\subsection{Likelihood}
Given $N$ data points with $N_k$ samples in class $k$:
\begin{equation}
L(\theta) = \prod_{i=1}^N P(y_i \mid \theta) = \prod_{k=1}^C \theta_k^{N_k}
\end{equation}

\subsection{Log-Likelihood}
\begin{equation}
\ell(\theta) = \log L(\theta) = \sum_{k=1}^C N_k \log \theta_k
\end{equation}

\subsection{Constraint}
\begin{equation}
\sum_{k=1}^C \theta_k = 1
\end{equation}

\subsection{Lagrangian}
\begin{equation}
\mathcal{L}(\theta, \lambda) = \sum_{k=1}^C N_k \log \theta_k - \lambda\left(\sum_{k=1}^C \theta_k - 1\right)
\end{equation}

\subsection{Optimization}
Take derivative with respect to $\theta_k$ and set to 0:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \theta_k} = \frac{N_k}{\theta_k} - \lambda = 0 \implies \theta_k = \frac{N_k}{\lambda}
\end{equation}

Use constraint to find $\lambda$:
\begin{equation}
\sum_{k=1}^C \theta_k = \sum_{k=1}^C \frac{N_k}{\lambda} = 1 \implies \lambda = \sum_{k=1}^C N_k = N
\end{equation}

\subsection{Final MLE}
\begin{equation}
\boxed{\theta_k^{\text{MLE}} = \frac{N_k}{N}}
\end{equation}

\section{MAP for Class Prior $\theta_k$}

\textbf{Goal:} Find $\theta_k$ that maximizes the posterior $P(\theta \mid D, \alpha)$.

\subsection{Prior: Dirichlet$(\alpha, \alpha, \ldots, \alpha)$}
\begin{equation}
P(\theta \mid \alpha) \propto \prod_{k=1}^C \theta_k^{\alpha-1}
\end{equation}

\subsection{Posterior (by Bayes' rule)}
\begin{align}
P(\theta \mid D, \alpha) &\propto P(D \mid \theta) \cdot P(\theta \mid \alpha) \\
&\propto \left[\prod_{k=1}^C \theta_k^{N_k}\right] \cdot \left[\prod_{k=1}^C \theta_k^{\alpha-1}\right] \\
&= \prod_{k=1}^C \theta_k^{N_k + \alpha - 1}
\end{align}

This is also a Dirichlet distribution: $\text{Dirichlet}(N_1 + \alpha, N_2 + \alpha, \ldots, N_C + \alpha)$

\subsection{MAP Estimate (Posterior Mean)}
For Dirichlet$(\alpha_1, \ldots, \alpha_C)$, the posterior mean is:
\begin{equation}
\theta_k = \frac{\alpha_k}{\sum_j \alpha_j}
\end{equation}

With our posterior Dirichlet$(N_k + \alpha, \ldots)$:
\begin{equation}
\boxed{\theta_k^{\text{MAP}} = \frac{N_k + \alpha}{\sum_{j=1}^C (N_j + \alpha)} = \frac{N_k + \alpha}{N + C \cdot \alpha}}
\end{equation}

\section{MLE for Pixel Prior $\theta_{i,k}$}

\textbf{Goal:} Find $\theta_{i,k} = P(x_i = 1 \mid y = k)$ for each pixel $i$ and class $k$.

\subsection{Setup}
\begin{itemize}
\item For class $k$, we have $N_k$ samples
\item Pixel $i$ is ``on'' ($x_i = 1$) in $C_{i,k}$ of those samples
\end{itemize}

\subsection{Likelihood for Pixel $i$ in Class $k$}
\begin{equation}
L(\theta_{i,k}) = \prod_{n: y_n=k} P(x_i^{(n)} \mid \theta_{i,k}) = \theta_{i,k}^{C_{i,k}} \cdot (1 - \theta_{i,k})^{N_k - C_{i,k}}
\end{equation}

\subsection{Log-Likelihood}
\begin{equation}
\ell(\theta_{i,k}) = C_{i,k} \log \theta_{i,k} + (N_k - C_{i,k}) \log(1 - \theta_{i,k})
\end{equation}

\subsection{Optimization}
Take derivative and set to 0:
\begin{align}
\frac{\partial \ell}{\partial \theta_{i,k}} &= \frac{C_{i,k}}{\theta_{i,k}} - \frac{N_k - C_{i,k}}{1 - \theta_{i,k}} = 0 \\
&\implies C_{i,k}(1 - \theta_{i,k}) = (N_k - C_{i,k})\theta_{i,k} \\
&\implies C_{i,k} = N_k \cdot \theta_{i,k}
\end{align}

\subsection{Final MLE}
\begin{equation}
\boxed{\theta_{i,k}^{\text{MLE}} = \frac{C_{i,k}}{N_k}}
\end{equation}

\textbf{Problem:} If $C_{i,k} = 0$, then $\theta_{i,k} = 0$, causing $P(x \mid y=k) = 0$ for any test sample where that pixel is on!

\section{MAP for Pixel Prior $\theta_{i,k}$}

\textbf{Goal:} Find $\theta_{i,k}$ that maximizes $P(\theta_{i,k} \mid D, \beta)$.

\subsection{Prior: Beta$(\beta, \beta)$}
\begin{equation}
P(\theta_{i,k} \mid \beta) \propto \theta_{i,k}^{\beta-1} \cdot (1 - \theta_{i,k})^{\beta-1}
\end{equation}

\subsection{Posterior}
\begin{align}
P(\theta_{i,k} \mid D, \beta) &\propto P(D \mid \theta_{i,k}) \cdot P(\theta_{i,k} \mid \beta) \\
&\propto \left[\theta_{i,k}^{C_{i,k}} \cdot (1-\theta_{i,k})^{N_k - C_{i,k}}\right] \cdot \left[\theta_{i,k}^{\beta-1} \cdot (1-\theta_{i,k})^{\beta-1}\right] \\
&= \theta_{i,k}^{C_{i,k} + \beta - 1} \cdot (1-\theta_{i,k})^{N_k - C_{i,k} + \beta - 1}
\end{align}

This is $\text{Beta}(C_{i,k} + \beta, N_k - C_{i,k} + \beta)$

\subsection{MAP Estimate (Posterior Mean)}
For Beta$(a, b)$, the posterior mean is:
\begin{equation}
\theta_{i,k} = \frac{a}{a + b}
\end{equation}

With our posterior Beta$(C_{i,k} + \beta, N_k - C_{i,k} + \beta)$:
\begin{align}
\theta_{i,k}^{\text{MAP}} &= \frac{C_{i,k} + \beta}{(C_{i,k} + \beta) + (N_k - C_{i,k} + \beta)} \\
&= \frac{C_{i,k} + \beta}{N_k + 2\beta}
\end{align}

\subsection{Final MAP}
\begin{equation}
\boxed{\theta_{i,k}^{\text{MAP}} = \frac{C_{i,k} + \beta}{N_k + 2\beta}}
\end{equation}

\section{Summary}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Parameter} & \textbf{MLE} & \textbf{MAP} \\
\hline
Class Prior $\theta_k$ & $\displaystyle\frac{N_k}{N}$ & $\displaystyle\frac{N_k + \alpha}{N + C \cdot \alpha}$ \\
\hline
Pixel Prior $\theta_{i,k}$ & $\displaystyle\frac{C_{i,k}}{N_k}$ & $\displaystyle\frac{C_{i,k} + \beta}{N_k + 2\beta}$ \\
\hline
\end{tabular}
\end{center}

\subsection{Key Insights}
\begin{itemize}
\item \textbf{MAP adds ``pseudo-counts'':} $\alpha$ for classes, $\beta$ for pixel states
\item When $\alpha, \beta \to 0$: MAP $\to$ MLE
\item When $\alpha, \beta \to \infty$: MAP $\to$ uniform ($1/C$ for classes, $0.5$ for pixels)
\item \textbf{Laplace smoothing} is the special case where $\alpha = \beta = 1$
\item MAP prevents zero probabilities, avoiding the ``zero-count problem'' of MLE
\end{itemize}

\end{document}
